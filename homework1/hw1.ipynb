{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import kruskal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
      "Reading from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\",\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\"\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for url in urls:\n",
    "    print(f\"Reading from {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    buffer = io.BytesIO(response.content)\n",
    "    df = pd.read_parquet(buffer, engine=\"pyarrow\") \n",
    "    df.columns = df.columns.str.lower()\n",
    "    dataframes.append(df)\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5980721, 19)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration_minutes'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of trip duration in January 2023: 42.59\n"
     ]
    }
   ],
   "source": [
    "std_duration=df[df['tpep_pickup_datetime'].dt.month == 1]['duration_minutes'].std()\n",
    "print(f\"Standard deviation of trip duration in January 2023: {std_duration:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction remaining: 98.12%\n"
     ]
    }
   ],
   "source": [
    "df_january = df[df['tpep_pickup_datetime'].dt.month == 1].copy()\n",
    "df_january = df_january[(df_january['duration_minutes'] >= 1) & (df_january['duration_minutes'] <= 60)]\n",
    "\n",
    "fraction_remaining = len(df_january) / len(df[df['tpep_pickup_datetime'].dt.month == 1])\n",
    "print(f\"Fraction remaining: {fraction_remaining:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (3009145, 515)\n"
     ]
    }
   ],
   "source": [
    "df_encoded = df_january[['pulocationid', 'dolocationid']].copy()\n",
    "df_encoded = df_encoded.astype(str)\n",
    "dicts = df_encoded.to_dict(orient='records')\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(dicts)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 7.65\n"
     ]
    }
   ],
   "source": [
    "# Target variable\n",
    "y_train = df_january['duration_minutes'].values\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and compute RMSE\n",
    "y_pred = model.predict(X_train)\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print(f\"Train RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Save model and vectorizer\n",
    "with open('model.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 7.81\n"
     ]
    }
   ],
   "source": [
    "with open('model.bin', 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)\n",
    "\n",
    "\n",
    "df_feb = df[df['tpep_pickup_datetime'].dt.month == 2].copy()\n",
    "df_feb = df_feb[(df_feb['duration_minutes'] >= 1) & (df_feb['duration_minutes'] <= 60)]\n",
    "\n",
    "\n",
    "df_val_encoded = df_feb[['pulocationid', 'dolocationid']].copy()\n",
    "df_val_encoded = df_val_encoded.astype(str)\n",
    "dicts_val = df_val_encoded.to_dict(orient='records')\n",
    "X_val = dv.transform(dicts_val)\n",
    "\n",
    "\n",
    "y_val = df_feb['duration_minutes'].values\n",
    "y_pred = model.predict(X_val)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"Validation RMSE: {rmse_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= df[df['tpep_pickup_datetime'].dt.month == 1].copy()\n",
    "df_test= df[df['tpep_pickup_datetime'].dt.month == 2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        # Time-based\n",
    "        df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "        df['pickup_dayofweek'] = df['tpep_pickup_datetime'].dt.dayofweek\n",
    "        df['pickup_weekend'] = (df['pickup_dayofweek'] >= 5).astype(int)\n",
    "        df['pickup_month'] = df['tpep_pickup_datetime'].dt.month\n",
    "\n",
    "        # Route & interaction\n",
    "        df['route'] = df['pulocationid'].astype(str) + '_' + df['dolocationid'].astype(str)\n",
    "        df['pulocationid_x_hour'] = df['pulocationid'].astype(str) + '_' + df['pickup_hour'].astype(str)\n",
    "        df['route_x_hour'] = df['route'] + '_' + df['pickup_hour'].astype(str)\n",
    "        df['payment_type_x_ratecodeid'] = df['payment_type'].astype(str) + '_' + df['ratecodeid'].astype(str)\n",
    "\n",
    "        # Speed & monetary ratios\n",
    "        df['trip_duration_sec'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds()\n",
    "        df['trip_speed_mph'] = df['trip_distance'] / (df['trip_duration_sec'] / 3600 + 1e-5)\n",
    "        df['tip_ratio'] = df['tip_amount'] / (df['fare_amount'] + 1e-5)\n",
    "        df['fare_per_mile'] = df['fare_amount'] / (df['trip_distance'] + 1e-5)\n",
    "        df['total_per_mile'] = df['total_amount'] / (df['trip_distance'] + 1e-5)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = TaxiFeatureEngineer()\n",
    "df_train = fe.transform(df_train.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "def calculate_woe_iv(df, feature, target, bins=10, eps=1e-6):\n",
    "    df = df[[feature, target]].dropna().copy()\n",
    "\n",
    "    # Bin numeric variables only\n",
    "    if is_numeric_dtype(df[feature]):\n",
    "        try:\n",
    "            df[feature] = pd.qcut(df[feature], q=bins, duplicates='drop')\n",
    "        except Exception:\n",
    "            return 0.0, pd.DataFrame()\n",
    "\n",
    "    # Group and calculate event/non-event counts\n",
    "    grouped = df.groupby(feature, observed=False)[target]\n",
    "    woe_df = grouped.agg(['count', 'sum']).rename(columns={'sum': 'event'})\n",
    "    woe_df['non_event'] = woe_df['count'] - woe_df['event']\n",
    "\n",
    "    total_events = woe_df['event'].sum()\n",
    "    total_non_events = woe_df['non_event'].sum()\n",
    "\n",
    "    if total_events == 0 or total_non_events == 0:\n",
    "        return 0.0, pd.DataFrame()\n",
    "\n",
    "    # Calculate smoothed rates\n",
    "    woe_df['event_rate'] = woe_df['event'] / total_events\n",
    "    woe_df['non_event_rate'] = woe_df['non_event'] / total_non_events\n",
    "\n",
    "    woe_df['event_rate'] = woe_df['event_rate'].replace(0, eps).fillna(eps)\n",
    "    woe_df['non_event_rate'] = woe_df['non_event_rate'].replace(0, eps).fillna(eps)\n",
    "\n",
    "    # Calculate WOE with warnings suppressed\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        ratio = (woe_df['event_rate'] + eps) / (woe_df['non_event_rate'] + eps)\n",
    "        ratio = ratio.replace([np.inf, -np.inf], eps).fillna(eps)\n",
    "        woe_df['woe'] = np.log(ratio)\n",
    "\n",
    "    woe_df['iv'] = (woe_df['event_rate'] - woe_df['non_event_rate']) * woe_df['woe']\n",
    "    iv = woe_df['iv'].sum()\n",
    "\n",
    "    return iv, woe_df[['woe', 'iv']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature\t\tIV\n",
      "------------------------------\n",
      "store_and_fwd_flag0.0000\n",
      "route           0.3603\n",
      "pulocationid_x_hour0.2425\n",
      "route_x_hour    0.3739\n",
      "payment_type_x_ratecodeid0.0720\n",
      "pulocationid    0.1288\n",
      "dolocationid    0.0768\n",
      "vendorid        0.0000\n",
      "passenger_count 0.0023\n",
      "ratecodeid      0.0000\n",
      "payment_type    0.0455\n",
      "mta_tax         0.0000\n",
      "improvement_surcharge0.0000\n",
      "congestion_surcharge0.0000\n",
      "airport_fee     0.1172\n",
      "pickup_weekend  0.0000\n"
     ]
    }
   ],
   "source": [
    "categorical_features = df_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "\n",
    "categorical_features = [col for col in categorical_features if col != 'duration_bin']\n",
    "categorical_features += ['pulocationid', 'dolocationid']\n",
    "df_train['duration_bin'] = pd.qcut(df_train['duration_minutes'], q=4, labels=False)\n",
    "\n",
    "\n",
    "for col in df_train.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    if df_train[col].nunique() < 50 and col not in ['duration_minutes', 'duration_bin']:\n",
    "        categorical_features.append(col)\n",
    "\n",
    "df_train['duration_bin'] = pd.qcut(df_train['duration_minutes'], q=4, labels=False)\n",
    "\n",
    "iv_results = []\n",
    "\n",
    "print(\"Feature\\t\\tIV\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for feature in categorical_features:\n",
    "    try:\n",
    "        iv, _ = calculate_woe_iv(df_train, feature, 'duration_bin')\n",
    "        iv_results.append((feature, iv))\n",
    "        print(f\"{feature:<16}{iv:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{feature:<16}Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Correlation with duration_minutes:\n",
      "                           pearson  spearman\n",
      "fare_amount                 0.2091    0.9211\n",
      "total_amount                0.2040    0.8891\n",
      "trip_distance               0.0039    0.8504\n",
      "total_per_mile             -0.0042   -0.6645\n",
      "fare_per_mile              -0.0032   -0.5240\n",
      "tip_amount                  0.1227    0.4218\n",
      "tolls_amount                0.1264    0.3734\n",
      "airport_fee                 0.1216    0.3554\n",
      "ratecodeid                     NaN    0.2667\n",
      "tip_ratio                  -0.0002   -0.2459\n",
      "trip_speed_mph             -0.0009    0.1856\n",
      "pulocationid_x_hour            NaN   -0.1301\n",
      "route                          NaN   -0.1289\n",
      "route_x_hour                   NaN   -0.1288\n",
      "pulocationid                   NaN   -0.1193\n",
      "dolocationid                   NaN   -0.1019\n",
      "congestion_surcharge       -0.0378   -0.1004\n",
      "payment_type                   NaN   -0.0671\n",
      "extra                       0.0224    0.0654\n",
      "improvement_surcharge       0.0091    0.0443\n",
      "store_and_fwd_flag             NaN    0.0438\n",
      "passenger_count                NaN    0.0349\n",
      "mta_tax                     0.0031    0.0344\n",
      "pickup_weekend                 NaN   -0.0303\n",
      "payment_type_x_ratecodeid      NaN    0.0290\n",
      "pickup_hour                    NaN    0.0135\n",
      "pickup_dayofweek               NaN   -0.0115\n",
      "vendorid                       NaN   -0.0031\n",
      "pickup_month                   NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# === Define feature groups ===\n",
    "pearson_features = [\n",
    "    'trip_distance',\n",
    "    'fare_amount',\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'airport_fee',\n",
    "    'trip_speed_mph',\n",
    "    'tip_ratio',\n",
    "    'fare_per_mile',\n",
    "    'total_per_mile'\n",
    "]\n",
    "\n",
    "spearman_features = [\n",
    "    'vendorid',\n",
    "    'passenger_count',\n",
    "    'ratecodeid',\n",
    "    'store_and_fwd_flag',\n",
    "    'pulocationid',\n",
    "    'dolocationid',\n",
    "    'payment_type',\n",
    "    'pickup_hour',\n",
    "    'pickup_dayofweek',\n",
    "    'pickup_weekend',\n",
    "    'pickup_month',\n",
    "    'route',\n",
    "    'pulocationid_x_hour',\n",
    "    'route_x_hour',\n",
    "    'payment_type_x_ratecodeid'\n",
    "]\n",
    "\n",
    "# Combine all for spearman\n",
    "all_features = list(set(pearson_features + spearman_features))\n",
    "\n",
    "# Make a safe copy\n",
    "df_corr = df_train.copy()\n",
    "\n",
    "# Label encode non-numeric spearman features\n",
    "for col in spearman_features:\n",
    "    if df_corr[col].dtype == 'object' or df_corr[col].dtype.name == 'category':\n",
    "        df_corr[col] = LabelEncoder().fit_transform(df_corr[col].astype(str))\n",
    "\n",
    "# === Calculate correlations ===\n",
    "\n",
    "# Pearson correlation (numeric-only)\n",
    "pearson_corr = df_corr[pearson_features + ['duration_minutes']].corr(method='pearson')['duration_minutes']\n",
    "\n",
    "# Spearman correlation (ordinal + label-encoded categoricals)\n",
    "spearman_corr = df_corr[pearson_features + spearman_features + ['duration_minutes']].corr(method='spearman')['duration_minutes']\n",
    "\n",
    "# === Combine into one DataFrame ===\n",
    "\n",
    "pearson_df = pd.DataFrame(pearson_corr).rename(columns={'duration_minutes': 'pearson'})\n",
    "spearman_df = pd.DataFrame(spearman_corr).rename(columns={'duration_minutes': 'spearman'})\n",
    "\n",
    "# Merge\n",
    "correlation_df = pearson_df.join(spearman_df, how='outer')\n",
    "correlation_df = correlation_df.drop(index='duration_minutes', errors='ignore')\n",
    "correlation_df = correlation_df[~correlation_df.index.duplicated()]\n",
    "correlation_df = correlation_df.sort_values(by='spearman', key=lambda x: x.abs(), ascending=False)\n",
    "\n",
    "# Print result\n",
    "print(\"üìä Correlation with duration_minutes:\")\n",
    "print(correlation_df.to_string(float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä ANOVA p-values (low-cardinality categorical features):\n",
      "ratecodeid                     0.0\n",
      "payment_type_x_ratecodeid      0.0\n",
      "payment_type                   1.1603530609144757e-187\n",
      "vendorid                       1.4475528627943033e-164\n",
      "store_and_fwd_flag             7.97553444401016e-11\n",
      "\n",
      "üå≤ Tree Feature Importances (high-cardinality categorical features):\n",
      "route                 0.5779\n",
      "route_x_hour          0.3359\n",
      "pulocationid_x_hour   0.0862\n"
     ]
    }
   ],
   "source": [
    "# === Step 0: Define target ===\n",
    "target = 'duration_minutes'\n",
    "\n",
    "# === Step 1: Detect categorical columns ===\n",
    "categorical_features = df_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Add manually known categorical ints\n",
    "explicit_cat_ints = ['payment_type', 'ratecodeid', 'vendorid']\n",
    "categorical_features += [col for col in explicit_cat_ints if col in df_train.columns]\n",
    "\n",
    "# Add engineered categorical features\n",
    "engineered_cats = [\n",
    "    'route', 'pulocationid_x_hour', 'route_x_hour', 'payment_type_x_ratecodeid'\n",
    "]\n",
    "categorical_features += [col for col in engineered_cats if col in df_train.columns]\n",
    "\n",
    "# Clean duplicates and exclude bins/targets\n",
    "categorical_features = list(set(categorical_features))\n",
    "categorical_features = [col for col in categorical_features if col not in ['duration_bin']]\n",
    "\n",
    "# === Step 2: Split by cardinality ===\n",
    "MAX_LEVELS = 100\n",
    "low_card_cats = [col for col in categorical_features if df_train[col].nunique() <= MAX_LEVELS]\n",
    "high_card_cats = [col for col in categorical_features if df_train[col].nunique() > MAX_LEVELS]\n",
    "\n",
    "# === Step 3: Run ANOVA for low-cardinality ===\n",
    "anova_results = {}\n",
    "for col in low_card_cats:\n",
    "    try:\n",
    "        model = smf.ols(f'{target} ~ C({col})', data=df_train).fit()\n",
    "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "        pval = anova_table[\"PR(>F)\"].iloc[0]\n",
    "        anova_results[col] = pval\n",
    "    except Exception as e:\n",
    "        anova_results[col] = f\"Error: {e}\"\n",
    "\n",
    "# === Step 4: Tree-based feature importance for high-cardinality ===\n",
    "\n",
    "# Encode high-card categorical features\n",
    "df_encoded = df_train[high_card_cats + [target]].dropna().copy()\n",
    "for col in high_card_cats:\n",
    "    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col].astype(str))\n",
    "\n",
    "# Train random forest\n",
    "X_tree = df_encoded[high_card_cats]\n",
    "y_tree = df_encoded[target]\n",
    "rf = RandomForestRegressor(n_estimators=50, max_depth=6, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_tree, y_tree)\n",
    "\n",
    "# Collect importances\n",
    "tree_importances = pd.Series(rf.feature_importances_, index=high_card_cats).sort_values(ascending=False)\n",
    "\n",
    "# === Step 5: Display results ===\n",
    "\n",
    "print(\"\\nüìä ANOVA p-values (low-cardinality categorical features):\")\n",
    "sorted_anova = sorted(anova_results.items(), key=lambda x: x[1] if isinstance(x[1], float) else 1)\n",
    "for feature, pval in sorted_anova:\n",
    "    print(f\"{feature:<30} {pval}\")\n",
    "\n",
    "print(\"\\nüå≤ Tree Feature Importances (high-cardinality categorical features):\")\n",
    "print(tree_importances.to_string(float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Pairwise T-Test p-values for categorical features (low cardinality only):\n",
      "\n",
      "vendorid:\n",
      "  2 vs 1: p = 0.0000e+00 ‚úÖ\n",
      "\n",
      "payment_type:\n",
      "  2 vs 1: p = 1.6126e-10 ‚úÖ\n",
      "  2 vs 4: p = 6.8061e-194 ‚úÖ\n",
      "  2 vs 3: p = 1.7951e-265 ‚úÖ\n",
      "  2 vs 0: p = 1.9533e-13 ‚úÖ\n",
      "  1 vs 4: p = 2.8162e-195 ‚úÖ\n",
      "  1 vs 3: p = 1.9669e-258 ‚úÖ\n",
      "  1 vs 0: p = 1.0499e-126 ‚úÖ\n",
      "  4 vs 3: p = 2.9472e-35 ‚úÖ\n",
      "  4 vs 0: p = 2.3896e-305 ‚úÖ\n",
      "  3 vs 0: p = 0.0000e+00 ‚úÖ\n",
      "\n",
      "store_and_fwd_flag:\n",
      "  N vs Y: p = 3.9435e-100 ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# === Step 0: Define target ===\n",
    "target = 'duration_minutes'\n",
    "\n",
    "# === Step 1: Identify object/category columns ===\n",
    "categorical_features = df_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# === Step 2: Add known numeric categoricals ===\n",
    "explicit_cat_ints = ['payment_type', 'ratecodeid', 'vendorid']\n",
    "categorical_features += [col for col in explicit_cat_ints if col in df_train.columns]\n",
    "\n",
    "# === Step 3: Add engineered categoricals (new features) ===\n",
    "engineered_cats = [\n",
    "    'route', 'pulocationid_x_hour', 'route_x_hour', 'payment_type_x_ratecodeid'\n",
    "]\n",
    "categorical_features += [col for col in engineered_cats if col in df_train.columns]\n",
    "\n",
    "# === Step 4: Deduplicate and filter for low-cardinality (‚â§ 6 groups) ===\n",
    "categorical_features = list(set(categorical_features))\n",
    "pairwise_features = [col for col in categorical_features if df_train[col].nunique() <= 6]\n",
    "\n",
    "# === Step 5: Efficient pairwise T-tests ===\n",
    "pairwise_results = defaultdict(list)\n",
    "\n",
    "for feature in pairwise_features:\n",
    "    col_data = df_train[[feature, target]].dropna()\n",
    "    groups = col_data[feature].unique()\n",
    "\n",
    "    for g1, g2 in combinations(groups, 2):\n",
    "        group1 = col_data[col_data[feature] == g1][target]\n",
    "        group2 = col_data[col_data[feature] == g2][target]\n",
    "\n",
    "        # Welch's t-test\n",
    "        t_stat, pval = ttest_ind(group1, group2, equal_var=False)\n",
    "        pairwise_results[feature].append((g1, g2, pval))\n",
    "\n",
    "# === Step 6: Display results ===\n",
    "print(\"\\nüìä Pairwise T-Test p-values for categorical features (low cardinality only):\")\n",
    "for feature, results in pairwise_results.items():\n",
    "    print(f\"\\n{feature}:\")\n",
    "    for g1, g2, pval in results:\n",
    "        sig = \"‚úÖ\" if pval < 0.05 else \"‚ùå\"\n",
    "        print(f\"  {g1} vs {g2}: p = {pval:.4e} {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Mutual Information Scores (Low & Medium Cardinality Features):\n",
      "                  Feature  MI_Score\n",
      "payment_type_x_ratecodeid    0.1311\n",
      "               ratecodeid    0.1173\n",
      "             payment_type    0.0204\n",
      "       store_and_fwd_flag    0.0097\n",
      "                 vendorid    0.0023\n",
      "\n",
      "‚ö†Ô∏è Skipped High Cardinality Features (Consider alternatives):\n",
      "  pulocationid              (257 unique) ‚Üí use RandomForest, Target Encoding, or Binning\n",
      "  dolocationid              (261 unique) ‚Üí use RandomForest, Target Encoding, or Binning\n",
      "  pulocationid_x_hour       (5111 unique) ‚Üí use RandomForest, Target Encoding, or Binning\n",
      "  route                     (22754 unique) ‚Üí use RandomForest, Target Encoding, or Binning\n",
      "  route_x_hour              (159394 unique) ‚Üí use RandomForest, Target Encoding, or Binning\n"
     ]
    }
   ],
   "source": [
    "# === Step 0: Define target ===\n",
    "target = 'duration_minutes'\n",
    "\n",
    "# === Step 1: Define all candidate categorical features ===\n",
    "categorical_features = [\n",
    "    'payment_type', 'ratecodeid', 'vendorid', 'store_and_fwd_flag',\n",
    "    'pulocationid', 'dolocationid',\n",
    "    'route', 'pulocationid_x_hour', 'route_x_hour', 'payment_type_x_ratecodeid'\n",
    "]\n",
    "\n",
    "# === Step 2: Check cardinality and categorize ===\n",
    "cardinality = {\n",
    "    col: df_train[col].nunique()\n",
    "    for col in categorical_features\n",
    "}\n",
    "\n",
    "cardinality_df = pd.DataFrame.from_dict(cardinality, orient='index', columns=['n_unique'])\n",
    "cardinality_df['cardinality_level'] = pd.cut(\n",
    "    cardinality_df['n_unique'],\n",
    "    bins=[0, 10, 100, float('inf')],\n",
    "    labels=['low', 'medium', 'high']\n",
    ")\n",
    "cardinality_df = cardinality_df.sort_values(by='n_unique')\n",
    "\n",
    "# === Step 3: Filter safe features for MI ===\n",
    "safe_mi_features = cardinality_df[cardinality_df['cardinality_level'].isin(['low', 'medium'])].index.tolist()\n",
    "high_card_features = cardinality_df[cardinality_df['cardinality_level'] == 'high'].index.tolist()\n",
    "\n",
    "# === Step 4: Prepare dataframe and encode only safe MI features ===\n",
    "df_cat = df_train[safe_mi_features + [target]].dropna(subset=[target]).copy()\n",
    "\n",
    "# Optional: Sample if very large\n",
    "MAX_ROWS = 200_000\n",
    "if len(df_cat) > MAX_ROWS:\n",
    "    df_cat = df_cat.sample(n=MAX_ROWS, random_state=42)\n",
    "\n",
    "# Label encode safe features\n",
    "for col in safe_mi_features:\n",
    "    df_cat[col] = LabelEncoder().fit_transform(df_cat[col].astype(str))\n",
    "\n",
    "X_cat = df_cat[safe_mi_features]\n",
    "y_cat = df_cat[target]\n",
    "\n",
    "# === Step 5: Compute MI ===\n",
    "mi_scores = mutual_info_regression(X_cat, y_cat, discrete_features=True)\n",
    "mi_df = pd.DataFrame({'Feature': safe_mi_features, 'MI_Score': mi_scores})\n",
    "mi_df = mi_df.sort_values(by='MI_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# === Step 6: Output results ===\n",
    "print(\"üìä Mutual Information Scores (Low & Medium Cardinality Features):\")\n",
    "print(mi_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Skipped High Cardinality Features (Consider alternatives):\")\n",
    "for col in high_card_features:\n",
    "    n = cardinality[col]\n",
    "    print(f\"  {col:<25} ({n} unique) ‚Üí use RandomForest, Target Encoding, or Binning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Kruskal‚ÄìWallis H-test Results (Top 50 values per feature):\n",
      "                  Feature  H_statistic     p_value  Significant\n",
      "               ratecodeid   1.8095e+04  0.0000e+00         True\n",
      "      pulocationid_x_hour   1.0259e+04  0.0000e+00         True\n",
      "payment_type_x_ratecodeid   2.0489e+04  0.0000e+00         True\n",
      "             dolocationid   1.0219e+04  0.0000e+00         True\n",
      "                    route   9.0662e+03  0.0000e+00         True\n",
      "             payment_type   1.6409e+03  0.0000e+00         True\n",
      "             pulocationid   2.8386e+04  0.0000e+00         True\n",
      "             route_x_hour   1.2573e+03 4.6183e-231         True\n",
      "                 vendorid   1.8166e+01  2.0249e-05         True\n",
      "       store_and_fwd_flag   1.1699e+00  2.7941e-01        False\n"
     ]
    }
   ],
   "source": [
    "# === Target ===\n",
    "target = 'duration_minutes'\n",
    "\n",
    "# === Define features ===\n",
    "\n",
    "# Features that passed MI test (medium cardinality)\n",
    "mi_passed_features = [\n",
    "    'payment_type_x_ratecodeid', 'ratecodeid', 'payment_type',\n",
    "    'store_and_fwd_flag', 'vendorid'\n",
    "]\n",
    "\n",
    "# High-cardinality features\n",
    "high_card_features = [\n",
    "    'pulocationid', 'dolocationid',\n",
    "    'pulocationid_x_hour', 'route', 'route_x_hour'\n",
    "]\n",
    "\n",
    "# Combine and ensure unique\n",
    "all_features = list(set(mi_passed_features + high_card_features))\n",
    "\n",
    "# === Optional: Sample for performance\n",
    "df_sample = df_train[all_features + [target]].dropna(subset=[target]).copy()\n",
    "if len(df_sample) > 200_000:\n",
    "    df_sample = df_sample.sample(200_000, random_state=42)\n",
    "\n",
    "# === Function to run Kruskal‚ÄìWallis Test ===\n",
    "def run_kruskal_test(df, feature, target, max_groups=50, min_group_size=20):\n",
    "    vc = df[feature].value_counts()\n",
    "    top_vals = vc[vc >= min_group_size].nlargest(max_groups).index\n",
    "    groups = [df[df[feature] == val][target] for val in top_vals]\n",
    "\n",
    "    # Require at least 2 groups\n",
    "    if len(groups) < 2:\n",
    "        return None, None\n",
    "\n",
    "    stat, pval = kruskal(*groups)\n",
    "    return stat, pval\n",
    "\n",
    "# === Run test for each feature ===\n",
    "results = []\n",
    "for feat in all_features:\n",
    "    stat, pval = run_kruskal_test(df_sample, feat, target)\n",
    "    if stat is not None:\n",
    "        results.append((feat, stat, pval))\n",
    "\n",
    "# === Format and display ===\n",
    "results_df = pd.DataFrame(results, columns=['Feature', 'H_statistic', 'p_value'])\n",
    "results_df['Significant'] = results_df['p_value'] < 0.05\n",
    "results_df = results_df.sort_values(by='p_value')\n",
    "\n",
    "print(\"üìä Kruskal‚ÄìWallis H-test Results (Top 50 values per feature):\")\n",
    "print(results_df.to_string(index=False, float_format=\"%.4e\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Variance Inflation Factor (VIF) for Numeric Features:\n",
      "              Feature    VIF\n",
      "                const  29.73\n",
      "        trip_distance   1.01\n",
      "          fare_amount 463.97\n",
      "                extra   2.70\n",
      "              mta_tax   4.27\n",
      "           tip_amount  24.17\n",
      "         tolls_amount   8.55\n",
      "improvement_surcharge   4.28\n",
      "         total_amount 723.61\n",
      " congestion_surcharge   2.59\n",
      "          airport_fee   2.63\n",
      "    trip_duration_sec   1.05\n",
      "       trip_speed_mph   1.00\n",
      "            tip_ratio   1.01\n",
      "        fare_per_mile  41.91\n",
      "       total_per_mile  41.22\n"
     ]
    }
   ],
   "source": [
    "numeric_features = [\n",
    "    'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount',\n",
    "    'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "    'congestion_surcharge', 'airport_fee',\n",
    "    'trip_duration_sec', 'trip_speed_mph',\n",
    "    'tip_ratio', 'fare_per_mile', 'total_per_mile'\n",
    "]\n",
    "\n",
    "\n",
    "# Prepare DataFrame: drop NaNs\n",
    "X_vif = df_train[numeric_features].dropna().copy()\n",
    "\n",
    "# Add constant (required by statsmodels)\n",
    "X_vif = add_constant(X_vif)\n",
    "\n",
    "# Compute VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X_vif.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Variance Inflation Factor (VIF) for Numeric Features:\")\n",
    "print(vif_data.to_string(index=False, float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ Final Feature Selection Summary\n",
    "\n",
    "## üìÇ Categorical Features (Selected)\n",
    "\n",
    "| Feature                  | IV     | MI Score | ANOVA P-value        | Kruskal H-test | T-Test    | Reason for Selection |\n",
    "|--------------------------|--------|----------|----------------------|----------------|-----------|-----------------------|\n",
    "| `pulocationid`           | 0.1288 | ‚Äî        | ‚Äî                    | ‚úÖ (28,386)     | ‚Äî         | High IV, Kruskal significant |\n",
    "| `dolocationid`           | 0.0768 | ‚Äî        | ‚Äî                    | ‚úÖ (10,219)     | ‚Äî         | High IV, Kruskal significant |\n",
    "| `ratecodeid`             | 0.0000 | 0.1173   | ‚úÖ (0.0)              | ‚úÖ              | ‚Äî         | Strong MI + Kruskal + ANOVA |\n",
    "| `payment_type`           | 0.0455 | 0.0204   | ‚úÖ (very low)         | ‚úÖ              | ‚úÖ         | Strong across all tests |\n",
    "| `store_and_fwd_flag`     | 0.0000 | 0.0097   | ‚úÖ (7.98e-11)         | ‚ùå              | ‚úÖ         | Low IV/MI but t-test significant |\n",
    "| `payment_type_x_ratecodeid` | 0.0720 | 0.1311 | ‚úÖ (0.0)           | ‚úÖ              | ‚Äî         | High MI + ANOVA + Kruskal |\n",
    "| `pulocationid_x_hour`    | 0.2425 | ‚Äî        | ‚Äî                    | ‚úÖ (10,259)     | ‚Äî         | High IV + tree importance + Kruskal |\n",
    "| `route`                  | 0.3603 | ‚Äî        | ‚Äî                    | ‚úÖ (9,066)      | ‚Äî         | High IV + tree importance |\n",
    "| `route_x_hour`           | 0.3739 | ‚Äî        | ‚Äî                    | ‚úÖ (1,257)      | ‚Äî         | Very high IV + tree importance |\n",
    "\n",
    "## üö´ Dropped Categorical Features\n",
    "\n",
    "| Feature              | IV     | MI Score | Reason for Dropping                         |\n",
    "|----------------------|--------|----------|---------------------------------------------|\n",
    "| `vendorid`           | 0.0000 | 0.0023   | Weak in IV + very low MI + low effect size |\n",
    "| `passenger_count`    | 0.0023 | ‚Äî        | Weak correlation and not stat sig overall   |\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Numerical Features (Selected)\n",
    "\n",
    "| Feature                | VIF   | Spearman Corr | Pearson Corr | Reason for Selection |\n",
    "|------------------------|-------|----------------|----------------|-----------------------|\n",
    "| `trip_distance`        | 1.01  | 0.8504         | 0.0039         | Very strong monotonic trend, low VIF |\n",
    "| `extra`                | 2.70  | 0.0654         | 0.0224         | Independent cost, low VIF |\n",
    "| `congestion_surcharge` | 2.59  | -0.1004        | -0.0378        | Weak inverse trend, low collinearity |\n",
    "| `airport_fee`          | 2.63  | 0.3554         | 0.1216         | Useful signal, no multicollinearity |\n",
    "\n",
    "## üö´ Dropped Numerical Features\n",
    "\n",
    "| Feature                  | VIF     | Reason for Dropping                                  |\n",
    "|--------------------------|---------|------------------------------------------------------|\n",
    "| `fare_amount`            | 463.97  | Redundant with total_amount, multicollinearity      |\n",
    "| `total_amount`           | 723.61  | Aggregated collinear metric, inflated VIF           |\n",
    "| `tip_amount`             | 24.17   | Overlaps with fare_amount, high VIF                 |\n",
    "| `tolls_amount`           | 8.55    | Optional for non-linear models, moderate VIF        |\n",
    "| `mta_tax`                | 4.27    | Very weak signal and small variance                 |\n",
    "| `improvement_surcharge` | 4.28    | Weak signal + moderate VIF                          |\n",
    "| `fare_per_mile`         | 41.91   | Correlated with fare and trip_distance              |\n",
    "| `total_per_mile`        | 41.22   | High VIF + redundancy with fare/total amount        |\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Final Feature Lists\n",
    "\n",
    "```python\n",
    "cat_features = [\n",
    "    'pulocationid', 'dolocationid', 'ratecodeid', 'payment_type',\n",
    "    'store_and_fwd_flag', 'payment_type_x_ratecodeid',\n",
    "    'pulocationid_x_hour', 'route', 'route_x_hour'\n",
    "]\n",
    "\n",
    "num_features = [\n",
    "    'trip_distance', 'extra', 'congestion_surcharge', 'airport_fee'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    'pulocationid', 'dolocationid', 'ratecodeid', 'payment_type',\n",
    "    'store_and_fwd_flag', 'payment_type_x_ratecodeid',\n",
    "    'pulocationid_x_hour', 'route', 'route_x_hour'\n",
    "]\n",
    "\n",
    "num_features = [\n",
    "    'trip_distance', 'extra', 'congestion_surcharge', 'airport_fee'\n",
    "]\n",
    "df_train= df_train[(df_train['duration_minutes'] >= 1) & (df_train['duration_minutes'] <= 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = cat_features + num_features + ['duration_minutes']\n",
    "\n",
    "# Prepare and clean data\n",
    "df_train_subset = df_train[selected_cols].dropna().copy()\n",
    "df_train_subset[cat_features] = df_train_subset[cat_features].astype(str)\n",
    "\n",
    "# Vectorize only categorical features as sparse\n",
    "dv = DictVectorizer(sparse=True)\n",
    "cat_dicts = df_train_subset[cat_features].to_dict(orient='records')\n",
    "X_cat = dv.fit_transform(cat_dicts)  # sparse matrix\n",
    "\n",
    "# Get numeric features\n",
    "X_num = df_train_subset[num_features].values\n",
    "\n",
    "# Combine sparse categorical and dense numeric\n",
    "X_train = sparse.hstack([X_cat, X_num]).tocsr()\n",
    "\n",
    "# Extract target\n",
    "y_train = df_train_subset['duration_minutes'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train RMSE: 4.32\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# 2. Train on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict on the same training set\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# 4. Evaluate with RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
    "print(f\"‚úÖ Train RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Test RMSE: 4.75\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test[(df_test['duration_minutes'] >= 1) & (df_test['duration_minutes'] <= 60)]\n",
    "\n",
    "\n",
    "df_test = fe.transform(df_test.copy())\n",
    "\n",
    "df_test_subset = df_test[selected_cols].dropna().copy()\n",
    "\n",
    "df_test_subset[cat_features] = df_test_subset[cat_features].astype(str)\n",
    "\n",
    "cat_dicts_test = df_test_subset[cat_features].to_dict(orient='records')\n",
    "X_cat_test = dv.transform(cat_dicts_test) \n",
    "\n",
    "X_num_test = df_test_subset[num_features].values\n",
    "\n",
    "X_test = sparse.hstack([X_cat_test, X_num_test]).tocsr()\n",
    "\n",
    "y_test = df_test_subset['duration_minutes'].values\n",
    "\n",
    "y_test_pred = model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"üß™ Test RMSE: {rmse_test:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
